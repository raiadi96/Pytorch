{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression using Pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMhHQ9ljsqVF4w0cSC4D1a5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raiadi96/Pytorch/blob/master/Linear_Regression_using_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQe6P-DFwA3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn #contains utility classes for building neural networks\n",
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbvNqg34y4kH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we create dummy inputs and their dummy targets and then convert them into to a tensor\n",
        "\n",
        "inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
        "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
        "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
        "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
        "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
        "                  dtype='float32')\n",
        "\n",
        "\n",
        "targets = np.array([[56, 70], [81, 101], [119, 133], \n",
        "                    [22, 37], [103, 119], [56, 70], \n",
        "                    [81, 101], [119, 133], [22, 37], \n",
        "                    [103, 119], [56, 70], [81, 101], \n",
        "                    [119, 133], [22, 37], [103, 119]], \n",
        "                   dtype='float32')\n",
        "\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm95DSio0F-e",
        "colab_type": "code",
        "outputId": "4ef1d7da-63ec-41ef-d97b-eb1e78d3eafc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "#in order to feed data to the neural network we first need to convert the data into a tensor dataset\n",
        "from torch.utils.data import TensorDataset\n",
        "train_ds = TensorDataset(inputs,targets)\n",
        "train_ds[0:5]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 69.,  96.,  70.]]), tensor([[ 56.,  70.],\n",
              "         [ 81., 101.],\n",
              "         [119., 133.],\n",
              "         [ 22.,  37.],\n",
              "         [103., 119.]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw1_7Ciy02RW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#it is important to split data into batches before being sent for training. this can be done using DataLoader. DataLoader also has utility for shuffling data\n",
        "from torch.utils.data import DataLoader\n",
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0NP4NTu06Ps",
        "colab_type": "code",
        "outputId": "f88ec9b4-8acc-4287-c7b7-6386b01d5f01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#we use nn.linear for creating the model. nn.linear will take 2 parameters in-shape and out-shape, inshape is the number of parameters that will be fed into the system and outshape is the shape of the output\n",
        "\n",
        "model = nn.Linear(3,2)\n",
        "print(model.weight)\n",
        "print(model.bias)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3947,  0.0448, -0.0688],\n",
            "        [ 0.4832, -0.2889, -0.2036]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1569, 0.1604], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbXoFm3j2phj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loss function\n",
        "#we use mean square error function in order to compute the loss\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#mse formula = 1/n*(ypred - ytarg)^2\n",
        "\n",
        "loss_fn = F.mse_loss\n",
        "\n",
        "#optimiser function\n",
        "#in order to optimise our results we use stochastic gradient descent. this allows for adjust the weights in a more optimised and effecient manner\n",
        "opt = torch.optim.SGD(model.parameters(), lr = 1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU0LJJTJDeiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#steps in training a model\n",
        "#for each epoch\n",
        "# 1. Generate Predictions\n",
        "# 2. Calculate the loss that occurs \n",
        "# 3. Compute the optimization metrics.\n",
        "# 4. Adjust the weights\n",
        "# 5. Reset the weights to zero\n",
        "\n",
        "def fit(num_epochs, model, loss_fn, opt):\n",
        "  for epoch in range(num_epochs):\n",
        "    for xb,yb in train_dl:\n",
        "      pred = model(xb)\n",
        "      loss = loss_fn(pred,yb)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      opt.zero_grad()\n",
        "\n",
        "    if (epoch+1)%10==0:\n",
        "      print(\"Epoch [{}/{}], Loss: {:.4f}\".format(epoch+1, num_epochs, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-22lesXEujt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bcf97d16-e41d-4a63-fd40-22915977c649"
      },
      "source": [
        "fit(2000, model, loss_fn, opt)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [10/2000], Loss: 0.6852\n",
            "Epoch [20/2000], Loss: 0.4698\n",
            "Epoch [30/2000], Loss: 0.5576\n",
            "Epoch [40/2000], Loss: 0.7282\n",
            "Epoch [50/2000], Loss: 0.6716\n",
            "Epoch [60/2000], Loss: 0.6038\n",
            "Epoch [70/2000], Loss: 0.7293\n",
            "Epoch [80/2000], Loss: 0.8270\n",
            "Epoch [90/2000], Loss: 0.8583\n",
            "Epoch [100/2000], Loss: 0.6019\n",
            "Epoch [110/2000], Loss: 0.5406\n",
            "Epoch [120/2000], Loss: 0.5924\n",
            "Epoch [130/2000], Loss: 0.8145\n",
            "Epoch [140/2000], Loss: 0.4567\n",
            "Epoch [150/2000], Loss: 0.5809\n",
            "Epoch [160/2000], Loss: 0.5658\n",
            "Epoch [170/2000], Loss: 0.7154\n",
            "Epoch [180/2000], Loss: 0.5956\n",
            "Epoch [190/2000], Loss: 0.5576\n",
            "Epoch [200/2000], Loss: 0.3406\n",
            "Epoch [210/2000], Loss: 0.6755\n",
            "Epoch [220/2000], Loss: 0.6474\n",
            "Epoch [230/2000], Loss: 0.3364\n",
            "Epoch [240/2000], Loss: 0.4879\n",
            "Epoch [250/2000], Loss: 0.3473\n",
            "Epoch [260/2000], Loss: 0.5199\n",
            "Epoch [270/2000], Loss: 0.4843\n",
            "Epoch [280/2000], Loss: 0.4013\n",
            "Epoch [290/2000], Loss: 0.3605\n",
            "Epoch [300/2000], Loss: 0.6465\n",
            "Epoch [310/2000], Loss: 0.5689\n",
            "Epoch [320/2000], Loss: 0.8362\n",
            "Epoch [330/2000], Loss: 0.4608\n",
            "Epoch [340/2000], Loss: 0.6143\n",
            "Epoch [350/2000], Loss: 0.4820\n",
            "Epoch [360/2000], Loss: 0.7296\n",
            "Epoch [370/2000], Loss: 0.6111\n",
            "Epoch [380/2000], Loss: 0.6354\n",
            "Epoch [390/2000], Loss: 0.6207\n",
            "Epoch [400/2000], Loss: 0.5755\n",
            "Epoch [410/2000], Loss: 0.6909\n",
            "Epoch [420/2000], Loss: 0.7298\n",
            "Epoch [430/2000], Loss: 0.4022\n",
            "Epoch [440/2000], Loss: 0.6335\n",
            "Epoch [450/2000], Loss: 0.3247\n",
            "Epoch [460/2000], Loss: 0.9718\n",
            "Epoch [470/2000], Loss: 0.4012\n",
            "Epoch [480/2000], Loss: 0.6209\n",
            "Epoch [490/2000], Loss: 0.5288\n",
            "Epoch [500/2000], Loss: 0.4561\n",
            "Epoch [510/2000], Loss: 0.5412\n",
            "Epoch [520/2000], Loss: 0.5640\n",
            "Epoch [530/2000], Loss: 0.7359\n",
            "Epoch [540/2000], Loss: 0.3309\n",
            "Epoch [550/2000], Loss: 0.4518\n",
            "Epoch [560/2000], Loss: 0.5443\n",
            "Epoch [570/2000], Loss: 0.5559\n",
            "Epoch [580/2000], Loss: 0.4787\n",
            "Epoch [590/2000], Loss: 0.5890\n",
            "Epoch [600/2000], Loss: 0.3866\n",
            "Epoch [610/2000], Loss: 0.7376\n",
            "Epoch [620/2000], Loss: 0.5700\n",
            "Epoch [630/2000], Loss: 0.4379\n",
            "Epoch [640/2000], Loss: 0.6525\n",
            "Epoch [650/2000], Loss: 0.4176\n",
            "Epoch [660/2000], Loss: 0.5720\n",
            "Epoch [670/2000], Loss: 1.0033\n",
            "Epoch [680/2000], Loss: 0.5310\n",
            "Epoch [690/2000], Loss: 0.4565\n",
            "Epoch [700/2000], Loss: 0.7213\n",
            "Epoch [710/2000], Loss: 0.4492\n",
            "Epoch [720/2000], Loss: 0.4788\n",
            "Epoch [730/2000], Loss: 0.6486\n",
            "Epoch [740/2000], Loss: 0.6876\n",
            "Epoch [750/2000], Loss: 0.6381\n",
            "Epoch [760/2000], Loss: 0.4548\n",
            "Epoch [770/2000], Loss: 0.2591\n",
            "Epoch [780/2000], Loss: 0.6801\n",
            "Epoch [790/2000], Loss: 0.4195\n",
            "Epoch [800/2000], Loss: 0.5212\n",
            "Epoch [810/2000], Loss: 0.7961\n",
            "Epoch [820/2000], Loss: 0.5212\n",
            "Epoch [830/2000], Loss: 0.5651\n",
            "Epoch [840/2000], Loss: 0.7202\n",
            "Epoch [850/2000], Loss: 0.5167\n",
            "Epoch [860/2000], Loss: 0.6005\n",
            "Epoch [870/2000], Loss: 0.6751\n",
            "Epoch [880/2000], Loss: 0.6186\n",
            "Epoch [890/2000], Loss: 0.6863\n",
            "Epoch [900/2000], Loss: 0.4305\n",
            "Epoch [910/2000], Loss: 0.5203\n",
            "Epoch [920/2000], Loss: 0.5352\n",
            "Epoch [930/2000], Loss: 0.5915\n",
            "Epoch [940/2000], Loss: 0.5771\n",
            "Epoch [950/2000], Loss: 0.3378\n",
            "Epoch [960/2000], Loss: 0.2649\n",
            "Epoch [970/2000], Loss: 0.5608\n",
            "Epoch [980/2000], Loss: 0.8112\n",
            "Epoch [990/2000], Loss: 0.3933\n",
            "Epoch [1000/2000], Loss: 0.5973\n",
            "Epoch [1010/2000], Loss: 0.5200\n",
            "Epoch [1020/2000], Loss: 0.6132\n",
            "Epoch [1030/2000], Loss: 0.6443\n",
            "Epoch [1040/2000], Loss: 0.6366\n",
            "Epoch [1050/2000], Loss: 0.6044\n",
            "Epoch [1060/2000], Loss: 0.5198\n",
            "Epoch [1070/2000], Loss: 0.6361\n",
            "Epoch [1080/2000], Loss: 0.6331\n",
            "Epoch [1090/2000], Loss: 0.3123\n",
            "Epoch [1100/2000], Loss: 0.6316\n",
            "Epoch [1110/2000], Loss: 0.3542\n",
            "Epoch [1120/2000], Loss: 0.4486\n",
            "Epoch [1130/2000], Loss: 0.5241\n",
            "Epoch [1140/2000], Loss: 0.6010\n",
            "Epoch [1150/2000], Loss: 0.7262\n",
            "Epoch [1160/2000], Loss: 0.7403\n",
            "Epoch [1170/2000], Loss: 0.5360\n",
            "Epoch [1180/2000], Loss: 0.6149\n",
            "Epoch [1190/2000], Loss: 0.8167\n",
            "Epoch [1200/2000], Loss: 0.4482\n",
            "Epoch [1210/2000], Loss: 0.5139\n",
            "Epoch [1220/2000], Loss: 0.4470\n",
            "Epoch [1230/2000], Loss: 0.6162\n",
            "Epoch [1240/2000], Loss: 0.3513\n",
            "Epoch [1250/2000], Loss: 0.6464\n",
            "Epoch [1260/2000], Loss: 0.4456\n",
            "Epoch [1270/2000], Loss: 0.5247\n",
            "Epoch [1280/2000], Loss: 0.5237\n",
            "Epoch [1290/2000], Loss: 0.4125\n",
            "Epoch [1300/2000], Loss: 0.5784\n",
            "Epoch [1310/2000], Loss: 0.5600\n",
            "Epoch [1320/2000], Loss: 0.5247\n",
            "Epoch [1330/2000], Loss: 0.4089\n",
            "Epoch [1340/2000], Loss: 0.7251\n",
            "Epoch [1350/2000], Loss: 0.6456\n",
            "Epoch [1360/2000], Loss: 0.7383\n",
            "Epoch [1370/2000], Loss: 0.3802\n",
            "Epoch [1380/2000], Loss: 0.5195\n",
            "Epoch [1390/2000], Loss: 0.5917\n",
            "Epoch [1400/2000], Loss: 0.5568\n",
            "Epoch [1410/2000], Loss: 0.6040\n",
            "Epoch [1420/2000], Loss: 0.5198\n",
            "Epoch [1430/2000], Loss: 0.5240\n",
            "Epoch [1440/2000], Loss: 0.4695\n",
            "Epoch [1450/2000], Loss: 0.5255\n",
            "Epoch [1460/2000], Loss: 0.6756\n",
            "Epoch [1470/2000], Loss: 0.5795\n",
            "Epoch [1480/2000], Loss: 0.5244\n",
            "Epoch [1490/2000], Loss: 0.5721\n",
            "Epoch [1500/2000], Loss: 0.5557\n",
            "Epoch [1510/2000], Loss: 0.3894\n",
            "Epoch [1520/2000], Loss: 0.4883\n",
            "Epoch [1530/2000], Loss: 0.5951\n",
            "Epoch [1540/2000], Loss: 0.5970\n",
            "Epoch [1550/2000], Loss: 0.4479\n",
            "Epoch [1560/2000], Loss: 0.6800\n",
            "Epoch [1570/2000], Loss: 0.4213\n",
            "Epoch [1580/2000], Loss: 0.5898\n",
            "Epoch [1590/2000], Loss: 0.5191\n",
            "Epoch [1600/2000], Loss: 0.3958\n",
            "Epoch [1610/2000], Loss: 0.5198\n",
            "Epoch [1620/2000], Loss: 0.5135\n",
            "Epoch [1630/2000], Loss: 0.5663\n",
            "Epoch [1640/2000], Loss: 0.3947\n",
            "Epoch [1650/2000], Loss: 0.5561\n",
            "Epoch [1660/2000], Loss: 0.5962\n",
            "Epoch [1670/2000], Loss: 0.5986\n",
            "Epoch [1680/2000], Loss: 0.3904\n",
            "Epoch [1690/2000], Loss: 0.4433\n",
            "Epoch [1700/2000], Loss: 0.3909\n",
            "Epoch [1710/2000], Loss: 0.4126\n",
            "Epoch [1720/2000], Loss: 0.5193\n",
            "Epoch [1730/2000], Loss: 0.5976\n",
            "Epoch [1740/2000], Loss: 0.5195\n",
            "Epoch [1750/2000], Loss: 0.4497\n",
            "Epoch [1760/2000], Loss: 0.5209\n",
            "Epoch [1770/2000], Loss: 0.6535\n",
            "Epoch [1780/2000], Loss: 0.5619\n",
            "Epoch [1790/2000], Loss: 0.4495\n",
            "Epoch [1800/2000], Loss: 0.6242\n",
            "Epoch [1810/2000], Loss: 0.5608\n",
            "Epoch [1820/2000], Loss: 0.6734\n",
            "Epoch [1830/2000], Loss: 0.5990\n",
            "Epoch [1840/2000], Loss: 0.6697\n",
            "Epoch [1850/2000], Loss: 0.7565\n",
            "Epoch [1860/2000], Loss: 0.7239\n",
            "Epoch [1870/2000], Loss: 0.5195\n",
            "Epoch [1880/2000], Loss: 0.5551\n",
            "Epoch [1890/2000], Loss: 0.6804\n",
            "Epoch [1900/2000], Loss: 0.5938\n",
            "Epoch [1910/2000], Loss: 0.6111\n",
            "Epoch [1920/2000], Loss: 0.3875\n",
            "Epoch [1930/2000], Loss: 0.2695\n",
            "Epoch [1940/2000], Loss: 0.3944\n",
            "Epoch [1950/2000], Loss: 0.5992\n",
            "Epoch [1960/2000], Loss: 0.3974\n",
            "Epoch [1970/2000], Loss: 0.4966\n",
            "Epoch [1980/2000], Loss: 0.5267\n",
            "Epoch [1990/2000], Loss: 0.5088\n",
            "Epoch [2000/2000], Loss: 0.4754\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTTxoEqrExZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#final predictions after 1000 epochs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvp7dkcfDD7h",
        "colab_type": "code",
        "outputId": "19396d7d-0cb9-4696-8096-e5bdaf8fc9b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "preds"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 57.2001,  70.3394],\n",
              "        [ 82.2024, 100.6572],\n",
              "        [118.7064, 132.9607],\n",
              "        [ 21.0857,  37.0167],\n",
              "        [101.9362, 119.1443],\n",
              "        [ 57.2001,  70.3394],\n",
              "        [ 82.2024, 100.6572],\n",
              "        [118.7064, 132.9607],\n",
              "        [ 21.0857,  37.0167],\n",
              "        [101.9362, 119.1443],\n",
              "        [ 57.2001,  70.3394],\n",
              "        [ 82.2024, 100.6572],\n",
              "        [118.7064, 132.9607],\n",
              "        [ 21.0857,  37.0167],\n",
              "        [101.9362, 119.1443]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d-s9EFjFRmB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "83d2594c-0c1f-41ca-c2ea-f0297d64e3be"
      },
      "source": [
        "#actual targets were\n",
        "targets"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 133.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.],\n",
              "        [ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 133.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.],\n",
              "        [ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 133.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsWMGbVvFWiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}