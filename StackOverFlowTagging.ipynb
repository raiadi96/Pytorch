{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StackOverFlowTagging.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOnOMsQu5n4WsGEZXjtai24",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raiadi96/Pytorch/blob/master/StackOverFlowTagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVUdgOI0WZQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr-jrYCdfCJu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "ef0925ee-fbc0-45cd-e83c-5f689324b49f"
      },
      "source": [
        "#fetch the dataset\n",
        "!tar -xzf stack_overflow_16k.tar.gz"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar (child): stack_overflow_16k.tar.gz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uem_octbft2h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c49c40f3-c1a0-4382-9185-9f440c994f2c"
      },
      "source": [
        "os.listdir()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'README.md', 'test', 'train', '.ipynb_checkpoints', 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrKUutLTgBAq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd49565c-a37c-46a9-f98a-723b7e9feace"
      },
      "source": [
        "train_dir = os.path.abspath('train')\n",
        "os.listdir(train_dir)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['javascript', 'csharp', 'java', 'python']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04DIcVXugGL9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a03c233c-4925-4c36-c098-4b97a0e6ba12"
      },
      "source": [
        "sample_file = os.path.join(train_dir, 'python/1991.txt')\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"how to automatically add unique key according to user input and then update it in blank i have a problem creating a dictionary key that is unique and comes from the user. and i don't know how to add it. so here is what i desire:..sale: blue 5.5 blue so far.sale: orange 2.2 orange so far.sale: blue 3.8 blue so far.sale: ...how do i update it?..object1 = {}..line = input('sales: ').while line:.  parts = line.split().  key = parts[0].  val = (parts[1])*1.  object1[key] = int(val).  print(object1).  line = input('sales: ')...firstly, i want to know how to add unique key to dictionary and second how do i add it.\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cixxLPahg_6s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bfe32f86-650a-4dc6-97d2-baa8636f5c39"
      },
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'train',\n",
        "    batch_size = batch_size,\n",
        "    validation_split = 0.2,\n",
        "    subset = 'training',\n",
        "    seed = seed\n",
        ")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 4 classes.\n",
            "Using 6400 files for training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amP1zdPIuQ5t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "600952a7-fc15-4b32-bc27-c72f9c0bfb22"
      },
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(1):\n",
        "    print(\"Question : {0}\".format(text_batch.numpy()[i]))\n",
        "    print(\"Label : {0}\".format(label_batch.numpy()[i]))\n",
        "    print(\"{0} is equivalent to {1}\".format(label_batch.numpy()[i], raw_train_ds.class_names[label_batch.numpy()[i]]))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question : b'\"unit testing of setters and getters teacher wanted us to do a comprehensive unit test. for me, this will be the first time that i use junit. i am confused about testing set and get methods. do you think should i test them? if the answer is yes; is this code enough for testing?..  public void testsetandget(){.    int a = 10;.    class firstclass = new class();.    firstclass.setvalue(10);.    int value = firstclass.getvalue();.    assert.asserttrue(\"\"error\"\", value==a);.  }...in my code, i think if there is an error, we can\\'t know that the error is deriving because of setter or getter.\"\\n'\n",
            "Label : 1\n",
            "1 is equivalent to java\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mbe-mqyu6YN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "430dc1d8-fa29-4ff9-c670-e96cf1691c8f"
      },
      "source": [
        "#similarly let us create validation and test dataset\n",
        "\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'train',\n",
        "    validation_split = 0.2,\n",
        "    batch_size = batch_size,\n",
        "    subset = 'validation',\n",
        "    seed = seed\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 4 classes.\n",
            "Using 1600 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yNJpfwqvokD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "737e4d39-b169-4c93-a91a-9cbc28f92500"
      },
      "source": [
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'test',\n",
        "    batch_size = batch_size\n",
        ")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tce4A32awB0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#standardization of text data\n",
        "import re\n",
        "def custom_standarization(text_data):\n",
        "  lowercase = tf.strings.lower(text_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />',' ')\n",
        "  return tf.strings.regex_replace(stripped_html,re.escape(string.punctuation), '')\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyu-DLN3w4Es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize = custom_standarization,\n",
        "    max_tokens = max_features,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = sequence_length\n",
        ")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6-sAPNzzwUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text = raw_train_ds.map(lambda x,y : x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4qL0rHl0b7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text,-1)\n",
        "  return vectorize_layer(text), label"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbhGKnEd1J72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmf2j6cb1M1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating a cache inorder to make sure data processing does not become a bottle neck while running a model\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE \n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size = AUTOTUNE)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdNfi6eD1RAv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "cddf2f2e-bea7-4c21-8252-7c8c6bba0274"
      },
      "source": [
        "embedding_dim = 16\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(4)])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 16)          160016    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 68        \n",
            "=================================================================\n",
            "Total params: 160,084\n",
            "Trainable params: 160,084\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK3NkDCF1VKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam', metrics=[\n",
        "  'accuracy'\n",
        "])\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoEJpHwZ1tTh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "39ec7b74-a3e4-42e6-ab85-dbac278f6b20"
      },
      "source": [
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 1.3789 - accuracy: 0.3461 - val_loss: 1.3676 - val_accuracy: 0.4050\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.3502 - accuracy: 0.4769 - val_loss: 1.3284 - val_accuracy: 0.5238\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.2983 - accuracy: 0.5342 - val_loss: 1.2681 - val_accuracy: 0.5619\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.2286 - accuracy: 0.5797 - val_loss: 1.1957 - val_accuracy: 0.6175\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.1513 - accuracy: 0.6311 - val_loss: 1.1208 - val_accuracy: 0.6694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU2rdXBB2QOu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "f0aab60c-25a0-4fda-d9bb-2d87bd490873"
      },
      "source": [
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250/250 [==============================] - 3s 12ms/step - loss: 1.1331 - accuracy: 0.6505\n",
            "Loss:  1.1330634355545044\n",
            "Accuracy:  0.6504999995231628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wOeiLt12gzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}